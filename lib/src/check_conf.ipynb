{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"usepool\": false, indicates that Connection Pooling is turned off. A sub-optimal deployment.\\n\"cacheEnabled\": false, = caching has been disabled. another sub-optimal configuration.\\nand finally the rules section must have a rule to enable r/w split;\\n    {\\n      \"enabled\": true,\\n      \"type\": \"V\",\\n      \"patterns\": [\\n        \"(?i)^select\"\\n      ],\\n      \"rowPatterns\": [],\\n      \"operator\": \"AND\",\\n      \"columnNameOperator\": \"AND\",\\n      \"intrans\": false,\\n      \"properties\": {}\\n    },\\n\\nall three of these being present confirms the proxy is configured \\nto support each of those features: connection pooling, caching, and r/w split.\\nThen the performance of each of those features can be examined.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\"usepool\": false, indicates that Connection Pooling is turned off. A sub-optimal deployment.\n",
    "\"cacheEnabled\": false, = caching has been disabled. another sub-optimal configuration.\n",
    "and finally the rules section must have a rule to enable r/w split;\n",
    "all three of these being present confirms the proxy is configured \n",
    "to support each of those features: connection pooling, caching, and r/w split.\n",
    "Then the performance of each of those features can be examined.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configHandler:\n",
    "    def __init__(\n",
    "        self, usepool_status=True, cacheStatus=True, rwStatus=True, authMode=True\n",
    "    ) -> None:\n",
    "        self.ups = usepool_status\n",
    "        self.cs = cacheStatus\n",
    "        self.rws = rwStatus\n",
    "        self.am = authMode\n",
    "\n",
    "    # builds a message that contains sanity check result. Returns empty string if passes check.\n",
    "    def get_msg(self) -> str:\n",
    "        content_to_send = []\n",
    "        if not self.ups:\n",
    "            content_to_send.append(\n",
    "                'Connection Pooling is turned off. (\"usepool\": false) in config.'\n",
    "            )\n",
    "        if not self.cs:\n",
    "            content_to_send.append(\n",
    "                'Caching is disabled. (\"cacheEnabled\": false) in config.'\n",
    "            )\n",
    "        if not self.rws:\n",
    "            content_to_send.append(\n",
    "                \"Read/write split is disabled. Check configuration file rule section.\"\n",
    "            )\n",
    "        if not self.am:\n",
    "            content_to_send.append(\n",
    "                \"Authentication mode is not 'passthrough', username and password need to be separately configured in the vdb.\"\n",
    "            )\n",
    "        return \"\\n\".join(content_to_send)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform basic sanity checks on a configuration file\n",
    "def sanity_check(conf: ps.sql.DataFrame) -> configHandler:\n",
    "    # check usepool\n",
    "    usepool_status = conf.select(\"sources\").collect()[0][\"sources\"][0][\"usepool\"]\n",
    "    # chceck cache\n",
    "    cacheStatus = conf.select(\"vdbs\").collect()[0][\"vdbs\"][0][\"cacheEnabled\"]\n",
    "    # check for proper r/w rules\n",
    "    rw_rules = conf.select(\"rules\").collect()[0][\"rules\"][0][\"rules\"]\n",
    "    rwStatus = False\n",
    "    for rule in rw_rules:\n",
    "        if \"(?i)^select\" in rule[\"patterns\"]:\n",
    "            rwStatus = True\n",
    "            break\n",
    "    # check whether authMode is passthrough \n",
    "    authMode = conf.select(\"vdbs\").collect()[0][\"vdbs\"][0][\"authMode\"] == 'passthrough'\n",
    "    return configHandler(usepool_status, cacheStatus, rwStatus, authMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Config Checker').getOrCreate()\n",
    "path = '../../data/sample_log_folders/talview-heimdall-logs-202309181309/core-01-vdb_1.conf'\n",
    "config = spark.read.option('multiLine', 'true').json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Authentication mode is not 'passthrough', username and password need to be separately configured in the vdb.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_handler = sanity_check(config)\n",
    "config_handler.get_msg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
